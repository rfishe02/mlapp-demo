{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a model for our application\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, get_cosine_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import pathlib\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a497b4a7fe246336\n",
      "Found cached dataset csv (/Users/renaef/.cache/huggingface/datasets/csv/default-a497b4a7fe246336/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b392357cd97c4e539282e59a91f6982a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/renaef/.cache/huggingface/datasets/csv/default-a497b4a7fe246336/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-60ed97895a9cb5f3.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/renaef/.cache/huggingface/datasets/csv/default-a497b4a7fe246336/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-14a16ae9c8240c30.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 269\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"./data/spotify_millsongdata.csv\")['train']\n",
    "dataset = dataset.filter(lambda example: 'abba' in example[\"artist\"].lower())\n",
    "dataset = dataset.remove_columns([\"artist\", \"song\", \"link\"])\n",
    "dataset = dataset.shuffle(seed=123)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 242\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 27\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = dataset.train_test_split(test_size=0.1)\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429e2652ffb141dbaf58e3f1facea272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6906cc2f9647108d23ef6f7b1cd11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5943ab6c0aa645239a31dfdbd98f179c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba925bc8c964e929570c4d71312438a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2', cache_dir=pathlib.Path('cache').resolve())\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True, num_proc=1, remove_columns=[\"text\"])\n",
    "block_size = tokenizer.model_max_length\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/renaef/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 97\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 52\n",
      "  Number of trainable parameters = 124439808\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e270384e81e3435b9ca13b4024dcc336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4212, 'learning_rate': 9.292589525111794e-05, 'epoch': 0.38}\n",
      "{'loss': 2.6389, 'learning_rate': 1.725216267546246e-05, 'epoch': 0.77}\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"output\"\n",
    "seed = 123\n",
    "LEARNING_RATE = 1.372e-4\n",
    "num_train_epochs = 4\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_total_limit=10,\n",
    "    save_strategy='epoch',\n",
    "    save_steps=1,\n",
    "    report_to=None,\n",
    "    seed=seed,\n",
    "    logging_steps=5,\n",
    "    do_eval=True,\n",
    "    eval_steps=1,\n",
    "    load_best_model_at_end=True\n",
    "    # disable_tqdm=True\n",
    "    # load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    # tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"test\"]\n",
    ")\n",
    "\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "num_train_steps = len(train_dataloader)\n",
    "trainer.create_optimizer_and_scheduler(num_train_steps)\n",
    "trainer.lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    trainer.optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "evaluation = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"I am\"\n",
    "num_sequences =  10\n",
    "min_length =  100\n",
    "max_length =   160\n",
    "temperature = 1\n",
    "top_p = 0.95\n",
    "top_k = 50\n",
    "repetition_penalty =  1.0\n",
    "\n",
    "encoded_prompt = tokenizer(start, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "encoded_prompt = encoded_prompt.to(trainer.model.device)\n",
    "\n",
    "# prediction\n",
    "output_sequences = trainer.model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    max_length=max_length,\n",
    "    min_length=min_length,\n",
    "    temperature=float(temperature),\n",
    "    top_p=float(top_p),\n",
    "    top_k=int(top_k),\n",
    "    do_sample=True,\n",
    "    repetition_penalty=repetition_penalty,\n",
    "    num_return_sequences=num_sequences)\n",
    "\n",
    "output_sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- Chapter 7: Deep Learning for Text, Deep Learning in Python by François Chollet\n",
    "- GPT-2, Accessed at: https://huggingface.co/gpt2\n",
    "- OpenAPI GPT-2, Accessed at: https://huggingface.co/docs/transformers/main/en/model_doc/gpt2\n",
    "- Fine-tune a pretrained model, Accessed at: https://huggingface.co/docs/transformers/training#train-in-native-pytorch\n",
    "- HuggingArtists - Train a model to generate lyrics, Accessed at: https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb\n",
    "- HuggingTweets - Train a model to generate tweets, Accessed at: https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb\n",
    "- Spotify Million Song Dataset, Accessed at: https://www.kaggle.com/datasets/notshrirang/spotify-million-song-dataset\n",
    "- Datasets, Accessed at: https://huggingface.co/docs/datasets/index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6859d0864aa76b03f74600b56e94b6efc7264cc44842d33771427805dad0a134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
