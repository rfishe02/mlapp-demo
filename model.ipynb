{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import TextVectorization, Dense, Embedding, GlobalAveragePooling1D\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import pickle\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 files belonging to 5 classes.\n",
      "Using 8 files for training.\n",
      "Found 10 files belonging to 5 classes.\n",
      "Using 2 files for validation.\n",
      "Found 5 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = \"./data/train\"\n",
    "test_data_dir = \"./data/test\"\n",
    "\n",
    "batch_size = 1024\n",
    "seed = 123\n",
    "\n",
    "''' \n",
    "Create batched datasets of the text and their labels. \n",
    "We'll use the commands validation_split and subset to split the training text into train and validation.\n",
    "'''\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(train_data_dir, batch_size=batch_size, validation_split=0.2, subset='training', seed=seed)\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(train_data_dir, batch_size=batch_size, validation_split=0.2, subset='validation', seed=seed)\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(test_data_dir, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 03:32:47.735641: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "\n",
    "text_vectorization = tf.keras.layers.TextVectorization(max_tokens=max_tokens, output_mode=\"int\", output_sequence_length=max_length,)\n",
    "\n",
    "# Prepare a dataset that only yeilds raw text, no labels.\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "# Use that dataset to index the dataset vocabulary via the adapt() method.\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# Prepare processed versions of our training, validation, & test dataset. Use num_parallel_calls to leverage multiple CPU cores.\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pre-trained Word Embeddings\n",
    "\n",
    "We can use one-hot vectors to represent text for machine learning purposes, which are sparse vectors that are mostly full of zeros and have a dimension equal to the number of words in a dataset or vocabulary. But, this approach isn't ideal for representing words because it requires a large amount of storage space. An alternative method is using word embeddings, which are dense vectors with fewer dimensions that still contain sufficient information for the machine learning process. This approach is more efficient and can be customized to a specific problem. Since we don't have enough data to build our own embeddings from scratch, we'll use embeddings from a model that has already been trained on a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_10 (Embedding)    (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirecti  (None, 64)               73984     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,194,049\n",
      "Trainable params: 5,194,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = tf.keras.layers.Embedding(input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs) \n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(embedded)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 03:32:50.991961: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 03:32:51.567320: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 03:32:51.740111: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 03:32:54.288788: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 03:32:54.446921: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - ETA: 0s - loss: 0.7082 - accuracy: 0.3750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 03:33:01.321502: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 03:33:01.538297: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 03:33:01.605120: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 16s 16s/step - loss: 0.7082 - accuracy: 0.3750 - val_loss: 0.0881 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 1s 715ms/step - loss: 0.2301 - accuracy: 0.1250 - val_loss: -0.5294 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 1s 701ms/step - loss: -0.1677 - accuracy: 0.1250 - val_loss: -1.3030 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 1s 829ms/step - loss: -0.5361 - accuracy: 0.1250 - val_loss: -2.1973 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 1s 669ms/step - loss: -1.1290 - accuracy: 0.1250 - val_loss: -3.1776 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 1s 646ms/step - loss: -1.9979 - accuracy: 0.1250 - val_loss: -4.1462 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 1s 644ms/step - loss: -2.4565 - accuracy: 0.1250 - val_loss: -4.9339 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 1s 731ms/step - loss: -3.0291 - accuracy: 0.1250 - val_loss: -5.6688 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 1s 634ms/step - loss: -3.2959 - accuracy: 0.1250 - val_loss: -6.2939 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 1s 693ms/step - loss: -4.5408 - accuracy: 0.1250 - val_loss: -6.8784 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17f7a46a0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_weights_filename = \"embeddings_bidir_gru_with_masking.keras\"\n",
    "\n",
    "# Create callbacks to save our best weights during training and log information we can use to visualize the training process in Tensorboard.\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(output_weights_filename, save_best_only=True),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "]\n",
    "\n",
    "# Train the model on the processesed training and validation datasets for 10 epochs.\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 03:33:11.731173: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 03:33:11.971125: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-09 03:33:12.044660: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step - loss: -4.5822 - accuracy: 0.2000\n",
      "Test acc: 0.200\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings we saved earlier and test it with our processed test data.\n",
    "model = tf.keras.models.load_model(output_weights_filename) \n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize performance\n",
    "\n",
    "Since we added a callback to Tensorboard to our training process, we can use the visualization capabilities of this tool to observe the results of our training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 48205), started 0:12:36 ago. (Use '!kill 48205' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-10b773ca2244e8db\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-10b773ca2244e8db\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- Chapter 7: Deep Learning for Text, Deep Learning in Python by François Chollet\n",
    "- Word embeddings, Accessed at: https://www.tensorflow.org/text/guide/word_embeddings\n",
    "- Word2Vec, Accessed at: https://www.tensorflow.org/tutorials/text/word2vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6859d0864aa76b03f74600b56e94b6efc7264cc44842d33771427805dad0a134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
