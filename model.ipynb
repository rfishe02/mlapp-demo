{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, TextVectorization\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"./data/train\"\n",
    "test_data_dir = \"./data/test\"\n",
    "\n",
    "batch_size = 1024\n",
    "seed = 123\n",
    "\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(train_data_dir, batch_size=batch_size, validation_split=0.2, subset='training', seed=seed)\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(train_data_dir, batch_size=batch_size, validation_split=0.2, subset='validation', seed=seed)\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(test_data_dir, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-08 18:02:21.676807: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "\n",
    "text_vectorization = tf.keras.layers.TextVectorization(max_tokens=max_tokens, output_mode=\"int\", output_sequence_length=max_length,)\n",
    "\n",
    "# Prepare a dataset that only yeilds raw text, no labels.\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "# Use that dataset to index the dataset vocabulary via the adapt() method.\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# Prepare processed versions of our training, validation, & test dataset. Use num_parallel_calls to leverage multiple CPU cores.\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent text for machine learning purposes using one-hot vectors, which are sparse vectors that are mostly full of zeros and have a dimension equal to the total number of words in the dataset or vocabulary. But, this approach isn't ideal for representing the relationships between words because it requires a large amount of storage and processing space. An alternative method is word embeddings, which are dense vectors with fewer dimensions that contain enough information for the machine learning process. This approach is more efficient and can be customized to a specific problem. Pre-trained embeddings are available, from models such as Word2Vec, though creating a custom embedding may yield better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = tf.keras.layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs) \n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))(embedded)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\", save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "\n",
    "model = tf.keras.models.load_model(\"embeddings_bidir_gru.keras\") \n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- Chapter 7: Deep Learning for Text - Deep Learning in Python by Fran√ßois Chollet\n",
    "- Word embeddings https://www.tensorflow.org/text/guide/word_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6859d0864aa76b03f74600b56e94b6efc7264cc44842d33771427805dad0a134"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
